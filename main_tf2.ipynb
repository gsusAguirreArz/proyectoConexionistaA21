{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf2': conda)"
  },
  "interpreter": {
   "hash": "6e09ea6a6242779ff345dd234e73a97e95e1ca80af4d0748ed5b3ae560e5f9ed"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1><center>Proyecto final</center></h1>\n",
    "\n",
    "\n",
    "<h2><center>Cómputo conexionista (A21)</center></h2>\n",
    "\n",
    "---\n",
    "\n",
    "<h3><center>\n",
    "\n",
    "José Ángel Avelar Barragan (A200361)\n",
    "\n",
    "José de Jesús Daniel Aguirre Arzate (A200350)\n",
    "</center></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<center><h2>\"Identification of patterns in cosmic-ray arrival directions using dynamic graph convolutional neural networks\"</center></h2>\n",
    "\n",
    "---\n",
    "\n",
    "Como proyecto final nos propusimos intentar entender el articulo y tratar de replicar los resultados. A la par intentamos relacionar los conceptos vistos en clase con lo que hace el articulo. Como primer objetivo tuvimos que entender como se estan procesando los datos y como objetivo final intentar construir la red que ellos proponen (tensorflow/pytorch)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2><center>Implementacion</center></h2>\n",
    "\n",
    "---\n",
    "\n",
    " \n",
    "- <h3>Tensorflow</h3>\n",
    "\n",
    "<h2><center>Teoria</center></h2>\n",
    "\n",
    "---\n",
    "\n",
    "**Problema:**\n",
    "A traves de los patrones en las direcciones de arrivo de las particulas, distingir rayos cosmicos que vienen de la misma fuente o de un fondo isotropico (ruido de fondo).\n",
    "\n",
    "Este metodo intenta encontrar patrones en las direcciones de arrivo de rayos cosmicos, si existen patrones se calsifica a las particulas como las que vienen de una fuente comun y las que vienen de direcciones de arribo isotropicas.\n",
    "\n",
    "Datos de entrenamiento: Simulaciones simples de direcciones de arribo donde se conoce la unica fuente que genera algunos rayos cosmicos(+ detalles en el articulo acerca de la configuracion del simulador BENCHMARK1)\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"700\" height=\"500\" src=\"images/skymap_0.png\">\n",
    "</p>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TensorFlow version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from astrotools import skymap\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "source": [
    "<h2><center>Preparacion de datos</center></h2>\n",
    "\n",
    "---\n",
    "\n",
    "El dataset *dataset_HAP.npz* esta organizado de la siquiente manera:\n",
    "\n",
    "Un diccionario con las keys: data, label\n",
    "En las cuales en data esta un arreglo de 50000 simulaciones de 500 particulas cada una con sus respectivas coordenadas (x,y,z, E) donde E es la energia normalizada. \n",
    "\n",
    "Y el arreglo que viene en label clasifica a 1 como aaquellas simulaciones que tienen una fuente o 0 como aquellas que solo son arrivos isotropicos. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    file = np.load(\"data/cosmic_ray_sphere/dataset_HAP.npz\")\n",
    "    x_train, x_test = file[\"data\"][:-10000], file[\"data\"][-10000:]\n",
    "    labels = tf.keras.utils.to_categorical(file[\"label\"],num_classes=2)\n",
    "    y_train, y_test = labels[:-10000], labels[-10000:]\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((40000, 500, 4), (10000, 500, 4))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "source": [
    "<h4>Sample</h4>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<Figure size 864x432 with 0 Axes>, <HammerAxes:>)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 864x432 with 0 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "test_id = 0\n",
    "example_map = X_test[test_id]\n",
    "skymap.eventmap(example_map[:,0:3].T, c=example_map[:,3], cblabel='Energy (normed)', opath=\"skymap_%i.png\" % test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_points, test_points, train_features, test_features = X_train[...,:3], X_test[...,:3], X_train[...,-1, np.newaxis], X_test[...,-1, np.newaxis]\n",
    "train_input_data, test_input_data = [train_points, train_features],[test_points, test_features]"
   ]
  },
  {
   "source": [
    "#############################################\n",
    "\n",
    "Aqui va teoria de GNN-EDgeCOnv\n",
    "\n",
    "#############################################"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 500, 3)]     0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 500, 1)]     0                                            \n__________________________________________________________________________________________________\nedge_conv (EdgeConv)            (None, 500, 8)       264         input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 500, 8)       0           edge_conv[0][0]                  \n__________________________________________________________________________________________________\nedge_conv_1 (EdgeConv)          (None, 500, 16)      1008        input_1[0][0]                    \n                                                                 activation[0][0]                 \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 500, 16)      0           edge_conv_1[0][0]                \n__________________________________________________________________________________________________\nedge_conv_2 (EdgeConv)          (None, 500, 32)      3552        input_1[0][0]                    \n                                                                 activation_1[0][0]               \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 500, 32)      0           edge_conv_2[0][0]                \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 500)          0           activation_2[0][0]               \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 2)            1002        global_average_pooling1d[0][0]   \n==================================================================================================\nTotal params: 5,826\nTrainable params: 5,490\nNon-trainable params: 336\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NODES=16\n",
    "\n",
    "LAYERS = tf.keras.layers\n",
    "def kernel_nn(data, nodes=NODES):\n",
    "    d1, d2 = data\n",
    "\n",
    "    delta = LAYERS.Subtract()([d1,d2])\n",
    "    x = LAYERS.Concatenate(axis=-1)([d1,delta])\n",
    "\n",
    "    x = LAYERS.Dense(nodes, activation=\"relu\")(x)\n",
    "    x = LAYERS.BatchNormalization()(x)\n",
    "\n",
    "    x = LAYERS.Dense(nodes, activation=\"relu\")(x)\n",
    "    x = LAYERS.BatchNormalization()(x)\n",
    "\n",
    "    x = LAYERS.Dense(nodes, activation=\"relu\")(x)\n",
    "    x = LAYERS.BatchNormalization()(x)\n",
    "    return x    \n",
    "\n",
    "points_input = LAYERS.Input((500,3))\n",
    "feats_input = LAYERS.Input((500,1))\n",
    "\n",
    "from edgeconv import EdgeConv\n",
    "x = EdgeConv(lambda a: kernel_nn(a, nodes=8), next_neighbors=5)([points_input, feats_input])\n",
    "x = LAYERS.Activation(\"relu\")(x)\n",
    "x = EdgeConv(lambda a: kernel_nn(a, nodes=16), next_neighbors=8)([points_input, x])\n",
    "x = LAYERS.Activation(\"relu\")(x)\n",
    "y = EdgeConv(lambda a: kernel_nn(a, nodes=32), next_neighbors=16)([points_input, x])\n",
    "x = LAYERS.Activation(\"relu\")(y)\n",
    "x = LAYERS.GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "out = LAYERS.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model([points_input, feats_input], out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(3E-3, decay=1E-4),metrics=['acc'])\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "hist = model.fit(train_input_data, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(history):\n",
    "    fig, axes = plt.subplots(2, figsize=(12,8))\n",
    "    if type(history) == dict:\n",
    "        loss = history[\"loss\"]\n",
    "        acc = history[\"acc\"]\n",
    "    else:\n",
    "        loss, acc = np.split(np.array(history), 2, axis=-1)\n",
    "    x = np.arange(len(loss))\n",
    "    axes[0].plot(x, loss, c=\"navy\")\n",
    "    axes[0].set_yscale(\"log\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[1].plot(x, acc, c=\"firebrick\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    if type(history) == dict:\n",
    "        axes[0].set_xlabel(\"Epochs\")\n",
    "        axes[1].set_xlabel(\"Epochs\")\n",
    "    else:\n",
    "        axes[0].set_xlabel(\"Iterations\")\n",
    "        axes[1].set_xlabel(\"Iterations\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_hist(hist.history)\n",
    "fig.savefig(\"./history.png\")"
   ]
  }
 ]
}